#!/usr/bin/env python3

import sys
import re
import string
from collections import defaultdict
from optparse import OptionParser
import os
import codecs
from nltk.parse import stanford
from nltk import sent_tokenize
from nltk import Tree
from nltk.tree import ParentedTree
import PCCParser
import csv


os.environ['STANFORD_PARSER'] = '../stanford-parser-full-2017-06-09' # TODO: set this in some config file
os.environ['STANFORD_MODELS'] = '../stanford-parser-full-2017-06-09'# same here
lexParser = stanford.StanfordParser(model_path="edu/stanford/nlp/models/lexparser/germanPCFG.ser.gz") # TODO: set this in some config file
os.environ['JAVAHOME'] = '/usr/lib/jvm/java-1.8.0-openjdk-amd64' # same here!
sanityList = []



class ConnectiveClassifier():

    def __init__(self):

        self.matrix = defaultdict(list)
        self.maxId = 0
        self.sanityList = []
        # TODO: read config file stuff

    

    def buildFeatureMatrixFromPCC(self, connectiveFiles):

        for i, cxml in enumerate(connectiveFiles):
            sys.stderr.write("INFO: Parsing file .../%s (%s/%s).\n" % (cxml.split('/')[-1], str(i+1), str(len(connectiveFiles))))
            pccTokens = PCCParser.parseConnectorFile(cxml)
            self.getFeaturesForPCCTokens(pccTokens)
        
    def getFeaturesForPCCTokens(self, pccTokens):        

        # NOTE: The bookkeeping here is a bit complicated, due to the conano inline xml format. Need to keep track of token id's to know if they are connective or not, and link this to the current node in the tree
        localId2class = defaultdict(bool)
        # ugly fix: filtering for parenthesis, as these are skipped by nltk.Tree traversal (probably because this symbol is used in internal representation)
        pccTokens = self.filterTokens(pccTokens)
        for i, t in enumerate(pccTokens):
            if t.isConnective:
                localId2class[i] = True
            else:
                localId2class[i] = False
        
        sentences = sent_tokenize(' '.join([re.sub(r'[()]+', '', t.token) for t in pccTokens])) # have to do the same ugly fix here because nltk.tree messed up any word with parenthesis in it
        tokenizedSents = [sent.split() for sent in sentences]
        try:
            forest = lexParser.parse_sents(tokenizedSents)
            tokenId = 0
            for trees in forest:
                for tree in trees:
                    featureVectors = self.getVectorsForTree(tree)
                    for fv in featureVectors:
                        nfv = list(fv)
                        nfv.append(localId2class[tokenId])
                        if (localId2class[tokenId]):
                            self.sanityList.append(nfv[0])
                        self.matrix[self.maxId] = nfv
                        self.maxId += 1
                        tokenId += 1
                    
        except ValueError:
            sys.stderr.write("WARNING: Failed to parse file. Skipping.\n")
            """
            # Got the following error for some file, coming from deep down the nltk parser somewhere...
            ValueError: Tree.read(): expected ')' but got 'end-of-string'
                at index 121.
                    "...JD 2.)))))"
            """

        
    
    def getVectorsForTree(self, tree):

        treeVectors = [] # returning a list of lists, as it takes a whole tree, and generates vectors for every word/node in the tree
        """
        Features per word (in order):
        - current word
        - pos tag of current word
        - previous word + current word
        - pos tag of previous word
        - previous word pos tag + current word pos tag
        - current word + next word
        - pos tag of next word
        - current word pos tag + next word pos tag
        - category of parent (in tree)
        - left sibling category (False if no left sibling)
        - right sibling category (False if no right sibling)
        - True if right sibling contains VP (False otherwise)
        # TODO: insert containsTrace here
        - path to root node (in categories)
        - compressed path to root node (in categories); identical adjacent categories compressed into one
        """

        parentedTree = ParentedTree.convert(tree)
        for i, node in enumerate(parentedTree.pos()):
            features = []
            currWord = node[0]
            currPos = node[1]
            features.append(currWord)
            features.append(currPos)
            ln = "SOS" if i == 0 else parentedTree.pos()[i-1]
            rn = "EOS" if i == len(parentedTree.pos())-1 else parentedTree.pos()[i+1]
            lpos = "_" if ln == "SOS" else ln[1]
            rpos = "_" if rn == "EOS" else rn[1]
            lstr = ln if ln == "SOS" else ln[0]
            rstr = rn if rn == "EOS" else rn[0]
            lbigram = lstr + '_' + currWord
            rbigram = currWord + '_' + rstr
            lposbigram = lpos + '_' + currPos
            rposbigram = currPos + '_' + rpos
            features.append(lbigram)
            features.append(lpos)
            features.append(lposbigram)
            features.append(rbigram)
            features.append(rpos)
            features.append(rposbigram)

            # TODO: self-category is not clear to me, if not always POS (because we are dealing with single words here in binary classification), what else should it be?
            nodePosition = parentedTree.leaf_treeposition(i)
            parent = parentedTree[nodePosition[:-1]].parent()
            parentCategory = parent.label()
            features.append(parentCategory)

            ls = parent.left_sibling()
            lsCat = False if not ls else ls.label()
            rs = parent.right_sibling()
            rsCat = False if not rs else rs.label()
            features.append(lsCat)
            features.append(rsCat)
            rsContainsVP = False
            if rs:
                if list(rs.subtrees(filter=lambda x: x.label()=='VP')):
                    rsContainsVP = True
            # TODO: Figure out how to check if rs contains a trace (given the tree/grammar)
            features.append(rsContainsVP)
            #featureList.append(rsContainsTrace) # TODO
            rootRoute = self.getPathToRoot(parent, [])
            features.append('_'.join(rootRoute))
            cRoute = self.compressRoute([x for x in rootRoute])
            features.append('_'.join(cRoute))

            treeVectors.append(features)

        return treeVectors

    def compressRoute(self, r): # filtering out adjacent identical tags
        
        delVal = "__DELETE__"
        for i in range(len(r)-1):
            if r[i] == r[i+1]:
                r[i+1] = delVal
        return [x for x in r if x != delVal]

    def getPathToRoot(self, ptree, route):

        if ptree.parent() == None:
            route.append(ptree.label())
            return route
        else:
            route.append(ptree.label())
            self.getPathToRoot(ptree.parent(), route)
        return route

    def filterTokens(self, tokens):
        skipSet = ['(', ')']
        return [t for t in tokens if not t.token in skipSet]



def getInputfiles(infolder):

    filelist = []
    for f in os.listdir(infolder):
        abspathFile = os.path.abspath(os.path.join(infolder, f))
        filelist.append(abspathFile)
    return filelist







"""    

def compressRoute(r): # filtering out adjacent identical tags

    delVal = "__DELETE__"
    for i in range(len(r)-1):
        if r[i] == r[i+1]:
            r[i+1] = delVal
    return [x for x in r if x != delVal]
    
            
def getPathToRoot(ptree, route):

    if ptree.parent() == None:
        route.append(ptree.label())
        return route
    else:
        route.append(ptree.label())
        getPathToRoot(ptree.parent(), route)
    return route
                     
            
def constructFeatureMatrix(connectorxmls):

    instanceIndex = 0
    instance2Values = defaultdict(list)
    
    for i, cxml in enumerate(connectorxmls):
        sys.stderr.write("INFO: Parsing file .../%s (%s/%s).\n" % (cxml.split('/')[-1], str(i+1), str(len(connectorxmls))))
        # extract real classes from PCC
        fileTokenList = PCCParser.parseConnectorFile(cxml)
        instance2Values, instanceIndex = getFeaturesForFileTokens(fileTokenList, instance2Values, instanceIndex)
    
    #for inst in instance2Values:
        #print(inst, instance2Values[inst])

    return instance2Values

def filterTokens(tokens):
    skipSet = ['(', ')']
    return [t for t in tokens if not t.token in skipSet]

def getFeaturesForFileTokens(ftokens, instance2Values, instanceIndex):        

    # bookkeeping
    id2class = defaultdict(bool)
    # ugly fix: filtering for parenthesis, as these are skipped by nltk.Tree traversal (probably because this symbol is used in internal representation)
    ftokens = filterTokens(ftokens)
    checkMaxBookId = 0
    for i, t in enumerate(ftokens):
        if t.isConnective:
            id2class[i + instanceIndex] = True
        else:
            id2class[i + instanceIndex] = False
        #print("debug book:", i, t.token)
        if i > checkMaxBookId:
            checkMaxBookId = i
    #TODO: once this is all working, refactor, as this bookkeeping stuff and the range() looping below is pretty unreadable.    

    sentences = sent_tokenize(' '.join([re.sub(r'[()]+', '', t.token) for t in ftokens])) # have to do the same ugly fix here because nltk.tree messed up any word with parenthesis in it
    tokenizedSents = [sent.split() for sent in sentences]
    prevIndex = instanceIndex
    try:
        forest = lexParser.parse_sents(tokenizedSents)
        for trees in forest:
            for tree in trees:
                instanceIndex, instance2Values = getFeaturesForTree(tree, instanceIndex, instance2Values)
        # add connective booleans (as last column)
        for j in range(prevIndex, instanceIndex):
            newl = list(instance2Values[j])
            newl.append(id2class[j])
            if id2class[j]:
                sanityList.append(newl[0])
            instance2Values[j] = newl
            
        # check if the indices match
        if instanceIndex - prevIndex - 1 != checkMaxBookId:
            sys.stderr.write("ERROR: Indices don't match up. Risk of misassigning. Dying now!")
            
    except ValueError:
        sys.stderr.write("WARNING: Failed to parse file. Skipping.\n")
        
        # Got the following error for some file, coming from deep down the nltk parser somewhere...
        #ValueError: Tree.read(): expected ')' but got 'end-of-string'
            #at index 121.
                #"...JD 2.)))))"
        

        
    return instance2Values, instanceIndex

        
def getFeaturesForTree(tree, instanceIndex, instance2Values):

    \"""
    Features (in order):
    - current word
    - pos tag of current word
    - previous word + current word
    - pos tag of previous word
    - previous word pos tag + current word pos tag
    - current word + next word
    - pos tag of next word
    - current word pos tag + next word pos tag
    - category of parent (in tree)
    - left sibling category (False if no left sibling)
    - right sibling category (False if no right sibling)
    - True if right sibling contains VP (False otherwise)
    - path to root node (in categories)
    - compressed path to root node (in categories); identical adjacent categories compressed into one
    \"""
    
    parentedTree = ParentedTree.convert(tree)
    for i, node in enumerate(parentedTree.pos()):
        #print("debug tree:", instanceIndex, node)
        featureList = []
        currWord = node[0]
        currPos = node[1]
        featureList.append(currWord)
        featureList.append(currPos)
        ln = "SOS" if i == 0 else parentedTree.pos()[i-1]
        rn = "EOS" if i == len(parentedTree.pos())-1 else parentedTree.pos()[i+1]
        lpos = "_" if ln == "SOS" else ln[1]
        rpos = "_" if rn == "EOS" else rn[1]
        lstr = ln if ln == "SOS" else ln[0]
        rstr = rn if rn == "EOS" else rn[0]
        lbigram = lstr + '_' + currWord
        rbigram = currWord + '_' + rstr
        lposbigram = lpos + '_' + currPos
        rposbigram = currPos + '_' + rpos
        featureList.append(lbigram)
        featureList.append(lpos)
        featureList.append(lposbigram)
        featureList.append(rbigram)
        featureList.append(rpos)
        featureList.append(rposbigram)
        
        
        # TODO: self-category is not clear to me, if not always POS (because we are dealing with single words here in binary classification), what else should it be?
        nodePosition = parentedTree.leaf_treeposition(i)
        parent = parentedTree[nodePosition[:-1]].parent()
        parentCategory = parent.label()
        featureList.append(parentCategory)
        
        ls = parent.left_sibling()
        lsCat = False if not ls else ls.label()
        rs = parent.right_sibling()
        rsCat = False if not rs else rs.label()
        featureList.append(lsCat)
        featureList.append(rsCat)
        rsContainsVP = False
        if rs:
            if list(rs.subtrees(filter=lambda x: x.label()=='VP')):
                rsContainsVP = True
        # TODO: Figure out how to check if rs contains a trace (given the tree/grammar)
        featureList.append(rsContainsVP)
        #featureList.append(rsContainsTrace) # TODO
        rootRoute = getPathToRoot(parent, []) # to make things a bit more general, not including the pos tag of the node itself here, as it is in plenty of other features already
        featureList.append('_'.join(rootRoute))
        cRoute = compressRoute([x for x in rootRoute])
        featureList.append('_'.join(cRoute))

        instance2Values[instanceIndex] = featureList
        instanceIndex += 1
        
    
    return instanceIndex, instance2Values
    
"""                

if __name__ == '__main__':

    parser = OptionParser('Usage: %prog -options')
    parser.add_option('-c', '--connectivesFolder', dest='connectivesFolder', help='Specify PCC connectives folder to construct a feature matrix for training a classifier.')
    parser.add_option('-v', '--verbose', dest='verbose', action='store_true', default=False, help='Include to get some debug info.')
    
    
    options, args = parser.parse_args()

    if not options.connectivesFolder:
        parser.print_help(sys.stderr)
        sys.exit(1)

    # MAJOR TODO: try this with the 2016 conll shared task code (on English) to see how this code compares to the scores on the shared task.
    
    #fm = constructFeatureMatrix(getInputfiles(options.connectivesFolder))

    cc = ConnectiveClassifier()
    cc.buildFeatureMatrixFromPCC(getInputfiles(options.connectivesFolder))
    fm = cc.matrix

    
    
    with open('tempout.csv', 'w') as csvfile:
        writer = csv.DictWriter(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL, fieldnames=['id', 'token', 'pos', 'leftbigram', 'leftpos', 'leftposbigram', 'rightbigram', 'rightpos', 'rightposbigram', 'parentCategory', 'leftSiblingCategory', 'rightSiblingCategory', 'rightSiblingContainsVP', 'pathToRoot', 'compressedPathToRoot', 'isConnective'])
        #writer.writeheader()
        for instance in fm:
            fVals = fm[instance]
            writer.writerow({'id': instance, 'token': fVals[0], 'pos': fVals[1], 'leftbigram': fVals[2], 'leftpos': fVals[3], 'leftposbigram': fVals[4], 'rightbigram': fVals[5], 'rightpos': fVals[6], 'rightposbigram': fVals[7], 'parentCategory': fVals[8], 'leftSiblingCategory': fVals[9], 'rightSiblingCategory': fVals[10], 'rightSiblingContainsVP': fVals[11], 'pathToRoot': fVals[12], 'compressedPathToRoot': fVals[13], 'isConnective': fVals[14]})



    if options.verbose:
        for item in sanityList:
            sys.stderr.write("VERBOSE: Added \t'%s'\t as connective.\n" % item)
